We are at a pivotal moment in the history of AI. The AI research community has driven progress for decades, but over the past couple years industry has started to make significant advances in model capabilities while purposely being closed about how. In this talk I'll start by discussing different types of contamination and how they appear in the wild. I'll then discuss some of our work on building massive datasets by scraping the web, including Dolma and C4. I'll discuss What's In My Big Data, a toolkit for documenting the contents of web-scale datasets, and some of our results on measuring contamination in different ways across a variety of popular pretraining corpora. I'll conclude by discussing evaluation of large models, and how current evaluations have low construct validity and how we don't have strong evaluations for the actual use cases that users care about.