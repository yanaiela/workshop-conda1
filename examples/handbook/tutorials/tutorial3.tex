\begin{center}
    \Large{\textbf{Interpretability and Analysis in Neural NLP}\\}
    \par\bigskip
    \large{Cutting-edge}\\
    \large{Yonatan Belinkov, Sebastian Gehrmann, and Ellie Pavlick}\\
    \par\bigskip
    [Website]

\end{center}

While deep learning has transformed the natural language processing (NLP) field and impacted the larger computational linguistics community, the rise of neural networks is stained by their opaque nature: It is challenging to
interpret the inner workings of neural network models, and explicate their behavior. Therefore, in the last few years,
an increasingly large body of work has been devoted to the analysis and interpretation of neural network models in
NLP. This body of work is so far lacking a common framework and methodology. Moreover, approaching the analysis
of modern neural networks can be difficult for newcomers to the field. This tutorial aims to fill this gap and introduce the nascent field of interpretability and analysis of neural networks in NLP. The tutorial will cover the main lines
of analysis work, such as structural analyses using probing classifiers, behavioral studies and test suites, and interactive visualizations. We will highlight not only the most commonly applied analysis methods, but also the specific
limitations and shortcomings of current approaches, in order to inform participants where to focus future efforts.

\begin{center}
    \noindent\rule{200px}{1pt}
\end{center}

\textbf{Yonatan Belinkov}, Postdoctoral Fellow, Harvard University and MIT\\

email: \texttt{belinkov@seas.harvard.edu}\\
website: \texttt{http://people.csail.mit.edu/belinkov}\\
Yonatan Belinkov is a Postdoctoral Fellow at the Harvard School of Engineering and Applied Sciences (SEAS) and the MIT Computer Science and Artificial Intelligence Laboratory (CSAIL). His research interests are in interpretability and robustness of neural models of language. He has done previous work in machine translation, speech recognition,community question answering, and syntactic parsing. His research has been published at ACL, EMNLP, NAACL, CL,TACL, ICLR, and NeurIPS. His PhD dissertation at MIT analyzed internal language representations in deep learning models. He co-organized or co-organizes BlackboxNLP 2019, BlackboxNLP 2020, and the WMT 2019 machine translation robustness task, and serves as an area chair for the analysis and interpretability track at ACL and EMNLP 2020.